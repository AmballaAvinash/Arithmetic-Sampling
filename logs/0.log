03/21/2024 06:25:14 - INFO - __main__ -   Script arguments:
03/21/2024 06:25:14 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:26:54 - INFO - __main__ -   Script arguments:
03/21/2024 06:26:54 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:27:09 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:30:15 - INFO - __main__ -   Script arguments:
03/21/2024 06:30:15 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:30:24 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:32:36 - INFO - __main__ -   Script arguments:
03/21/2024 06:32:36 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:32:46 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:37:36 - INFO - __main__ -   Script arguments:
03/21/2024 06:37:36 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:37:46 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:41:33 - INFO - __main__ -   Script arguments:
03/21/2024 06:41:33 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:41:42 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:44:49 - INFO - __main__ -   Script arguments:
03/21/2024 06:44:49 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:44:58 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:46:07 - INFO - __main__ -   Script arguments:
03/21/2024 06:46:07 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:46:16 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:47:28 - INFO - __main__ -   Script arguments:
03/21/2024 06:47:28 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:47:38 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:49:30 - INFO - __main__ -   Script arguments:
03/21/2024 06:49:30 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:49:39 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:51:36 - INFO - __main__ -   Script arguments:
03/21/2024 06:51:36 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:51:46 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:51:53 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'zeroshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'static', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/21/2024 06:55:58 - INFO - __main__ -   Script arguments:
03/21/2024 06:55:58 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 06:56:08 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 06:56:21 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'zeroshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/21/2024 06:56:21 - INFO - __main__ -   `sample_answer` not found in example while using use_answer=True mode
03/21/2024 06:56:21 - INFO - __main__ -   Exiting...
03/21/2024 07:05:24 - INFO - __main__ -   Script arguments:
03/21/2024 07:05:24 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 07:05:33 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 07:05:48 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'zeroshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/21/2024 07:12:46 - INFO - __main__ -   Script arguments:
03/21/2024 07:12:46 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/21/2024 07:12:56 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/21/2024 07:13:22 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'zeroshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/21/2024 07:45:24 - INFO - __main__ -   Sampling generations (strategy=eta):
03/22/2024 02:55:02 - INFO - __main__ -   Script arguments:
03/22/2024 02:55:02 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'zeroshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 02:55:22 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 03:00:14 - INFO - __main__ -   Script arguments:
03/22/2024 03:00:14 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 03:00:27 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 03:03:55 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 03:43:28 - INFO - __main__ -   Script arguments:
03/22/2024 03:43:28 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 03:43:36 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 03:44:18 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 03:44:48 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 03:48:30 - INFO - __main__ -   Script arguments:
03/22/2024 03:48:30 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 03:48:35 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 03:48:45 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 03:49:12 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 03:54:33 - INFO - __main__ -   Script arguments:
03/22/2024 03:54:33 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 03:54:39 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 03:55:43 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 03:55:47 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 04:05:37 - INFO - __main__ -   Script arguments:
03/22/2024 04:05:37 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:05:43 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:05:51 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 04:05:57 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 04:07:29 - INFO - __main__ -   Script arguments:
03/22/2024 04:07:29 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:07:34 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:07:43 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 04:07:44 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 04:35:14 - INFO - __main__ -   Script arguments:
03/22/2024 04:35:14 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:35:20 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:40:36 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 04:40:38 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 04:43:51 - INFO - __main__ -   Script arguments:
03/22/2024 04:43:51 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:43:56 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:47:10 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 04:47:12 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 04:48:52 - INFO - __main__ -   Script arguments:
03/22/2024 04:48:52 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:48:57 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:49:18 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 04:49:25 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 04:52:57 - INFO - __main__ -   Script arguments:
03/22/2024 04:52:57 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:53:04 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:53:12 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 04:53:13 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 04:58:01 - INFO - __main__ -   Script arguments:
03/22/2024 04:58:01 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:58:06 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:59:05 - INFO - __main__ -   Script arguments:
03/22/2024 04:59:05 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 04:59:10 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 04:59:47 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:00:07 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 05:02:31 - INFO - __main__ -   Script arguments:
03/22/2024 05:02:31 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 05:02:37 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 05:03:06 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:03:08 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 05:05:11 - INFO - __main__ -   Script arguments:
03/22/2024 05:05:11 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 05:05:16 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 05:07:13 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:07:15 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 05:16:19 - INFO - __main__ -   Script arguments:
03/22/2024 05:16:19 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 05:16:26 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 05:16:35 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:16:35 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 05:19:38 - INFO - __main__ -   Script arguments:
03/22/2024 05:19:38 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 05:19:43 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 05:19:48 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:19:48 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 05:22:23 - INFO - __main__ -   Script arguments:
03/22/2024 05:22:23 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 05:22:29 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 05:22:34 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:22:34 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 05:24:24 - INFO - __main__ -   Script arguments:
03/22/2024 05:24:24 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'google/gemma-2b', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 05:24:30 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 05:24:35 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:24:35 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 05:44:51 - INFO - __main__ -   Script arguments:
03/22/2024 05:44:51 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 05:44:55 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 05:45:00 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 05:45:00 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 07:34:25 - INFO - __main__ -   Script arguments:
03/22/2024 07:34:25 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 07:34:29 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 07:34:34 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 07:34:34 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:03:48 - INFO - __main__ -   Script arguments:
03/22/2024 08:03:48 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:03:52 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:03:56 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:03:56 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:06:41 - INFO - __main__ -   Script arguments:
03/22/2024 08:06:41 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:06:45 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:06:50 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:06:50 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:09:25 - INFO - __main__ -   Script arguments:
03/22/2024 08:09:25 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:09:29 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:09:33 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:09:33 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:14:43 - INFO - __main__ -   Script arguments:
03/22/2024 08:14:43 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:14:47 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:14:52 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:14:52 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:15:06 - INFO - __main__ -   Example #1:
03/22/2024 08:15:06 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:15:06 - INFO - __main__ -   Gold: yes
03/22/2024 08:15:06 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2', '1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2']
03/22/2024 08:15:06 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:15:55 - INFO - __main__ -   Script arguments:
03/22/2024 08:15:55 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:15:59 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:16:03 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:16:03 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:16:31 - INFO - __main__ -   Example #1:
03/22/2024 08:16:31 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:16:31 - INFO - __main__ -   Gold: yes
03/22/2024 08:16:31 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2', '1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2']
03/22/2024 08:16:31 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:17:59 - INFO - __main__ -   Script arguments:
03/22/2024 08:17:59 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:18:03 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:18:07 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:18:07 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:18:28 - INFO - __main__ -   Example #1:
03/22/2024 08:18:28 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:18:28 - INFO - __main__ -   Gold: yes
03/22/2024 08:18:28 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2 |\nAnswer 1: Yes\nReasons: 2 + 2 = 4\nAnswer to Question 2: Yes.\n2 +2 =4\nReferences\n1. <http://www.mathsisfun.com/sets/comparing-numbers.html>\n2. <https://www2.gsu.edu/~wwwesl/eslnotes/numbers.htm>\n3. <www.dummies.com>', '1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2 |\nAnswer 1: Yes\nReasons: 2 + 2 = 4\nAnswer to Question 2: Yes.\n2 +2 =4\nReferences\n1. <http://www.mathsisfun.com/sets/comparing-numbers.html>\n2. <https://www2.gsu.edu/~wwwesl/eslnotes/numbers.htm>\n3. <www.dummies.com>']
03/22/2024 08:18:28 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:19:04 - INFO - __main__ -   Example #2:
03/22/2024 08:19:04 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Does ontology require a scalpel?
Reasoning: 
03/22/2024 08:19:04 - INFO - __main__ -   Gold: no
03/22/2024 08:19:04 - INFO - __main__ -   Predictions: ['1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, 4, and 5 are all true. Thus ontology requires a scalpell. So, the answer to the question is yes, ontology does require a surgery.\nLike the previous example, answer each of these questions with Yes, No, or Maybe.\n1. Is the number of people who have died from COVID-19 greater than the number who have been born in the United States?\n2. Is it possible to have a square circle?\n3. Is there a difference between a pig and a boar?\n4. Is a pineapple a fruit?\n5', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, 4, and 5 are all true. Thus ontology requires a scalpell. So, the answer to the question is yes, ontology does require a surgery.\nLike the previous example, answer each of these questions with Yes, No, or Maybe.\n1. Is the number of people who have died from COVID-19 greater than the number who have been born in the United States?\n2. Is it possible to have a square circle?\n3. Is there a difference between a pig and a boar?\n4. Is a pineapple a fruit?\n5']
03/22/2024 08:20:53 - INFO - __main__ -   Script arguments:
03/22/2024 08:20:53 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:20:57 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:21:01 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': None, 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:21:01 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:21:21 - INFO - __main__ -   Example #1:
03/22/2024 08:21:21 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:21:21 - INFO - __main__ -   Gold: yes
03/22/2024 08:21:21 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2 |\nAnswer 1: Yes\nReasons: 2 + 2 = 4\nAnswer to Question 2: Yes.\n2 +2 =4\nReferences\n1. <http://www.mathsisfun.com/sets/comparing-numbers.html>\n2. <https://www2.gsu.edu/~wwwesl/eslnotes/numbers.htm>\n3. <www.dummies.com>', '1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2 |\nAnswer 1: Yes\nReasons: 2 + 2 = 4\nAnswer to Question 2: Yes.\n2 +2 =4\nReferences\n1. <http://www.mathsisfun.com/sets/comparing-numbers.html>\n2. <https://www2.gsu.edu/~wwwesl/eslnotes/numbers.htm>\n3. <www.dummies.com>']
03/22/2024 08:21:21 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:21:53 - INFO - __main__ -   Example #2:
03/22/2024 08:21:53 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Does ontology require a scalpel?
Reasoning: 
03/22/2024 08:21:53 - INFO - __main__ -   Gold: no
03/22/2024 08:21:53 - INFO - __main__ -   Predictions: ['1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, 4, and 5 are all true. Thus ontology requires a scalpell. So, the answer to the question is yes, ontology does require a surgery.\nLike the previous example, answer each of these questions with Yes, No, or Maybe.\n1. Is the number of people who have died from COVID-19 greater than the number who have been born in the United States?\n2. Is it possible to have a square circle?\n3. Is there a difference between a pig and a boar?\n4. Is a pineapple a fruit?\n5', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, 4, and 5 are all true. Thus ontology requires a scalpell. So, the answer to the question is yes, ontology does require a surgery.\nLike the previous example, answer each of these questions with Yes, No, or Maybe.\n1. Is the number of people who have died from COVID-19 greater than the number who have been born in the United States?\n2. Is it possible to have a square circle?\n3. Is there a difference between a pig and a boar?\n4. Is a pineapple a fruit?\n5']
03/22/2024 08:22:04 - INFO - __main__ -   {'accuracy.accuracy': 0.5}
03/22/2024 08:25:37 - INFO - __main__ -   Script arguments:
03/22/2024 08:25:37 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 2, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:25:41 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:25:45 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 2, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': '', 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:25:45 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:26:06 - INFO - __main__ -   Example #1:
03/22/2024 08:26:06 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:26:06 - INFO - __main__ -   Gold: yes
03/22/2024 08:26:06 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2 |\nAnswer 1: Yes\nReasons: 2 + 2 = 4\nAnswer to Question 2: Yes.\n2 +2 =4\nReferences\n1. <http://www.mathsisfun.com/sets/comparing-numbers.html>\n2. <https://www2.gsu.edu/~wwwesl/eslnotes/numbers.htm>\n3. <www.dummies.com>', '1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is yes, Gandalf would defeat Rincewin.\nRefer to the following table for the answers to the questions.\n| Question | Answer | Reasoning |\n| 1 | Yes | 1 + 1 = 2 |\nAnswer 1: Yes\nReasons: 2 + 2 = 4\nAnswer to Question 2: Yes.\n2 +2 =4\nReferences\n1. <http://www.mathsisfun.com/sets/comparing-numbers.html>\n2. <https://www2.gsu.edu/~wwwesl/eslnotes/numbers.htm>\n3. <www.dummies.com>']
03/22/2024 08:26:06 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:26:19 - INFO - __main__ -   Example #2:
03/22/2024 08:26:19 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Does ontology require a scalpel?
Reasoning: 
03/22/2024 08:26:19 - INFO - __main__ -   Gold: no
03/22/2024 08:26:19 - INFO - __main__ -   Predictions: ['1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, 4, and 5 are all true. Thus ontology requires a scalpell. So, the answer to the question is yes, ontology does require a surgery.\nLike the previous example, answer each of these questions with Yes, No, or Maybe.\n1. Is the number of people who have died from COVID-19 greater than the number who have been born in the United States?\n2. Is it possible to have a square circle?\n3. Is there a difference between a pig and a boar?\n4. Is a pineapple a fruit?\n5', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, 4, and 5 are all true. Thus ontology requires a scalpell. So, the answer to the question is yes, ontology does require a surgery.\nLike the previous example, answer each of these questions with Yes, No, or Maybe.\n1. Is the number of people who have died from COVID-19 greater than the number who have been born in the United States?\n2. Is it possible to have a square circle?\n3. Is there a difference between a pig and a boar?\n4. Is a pineapple a fruit?\n5']
03/22/2024 08:26:19 - INFO - __main__ -   {'accuracy.accuracy': 0.5}
03/22/2024 08:26:19 - INFO - __main__ -   Saved results to outputs/outputs/self_consistency/strategy_qa/eval_llama-2-7b-hf_strategy_qa__validation_2_fewshot_arithmetic_22_03_08_26_0/results.json
03/22/2024 08:26:19 - INFO - __main__ -   Sampling generations (strategy=temperature):
03/22/2024 08:26:27 - INFO - __main__ -   Example #1:
03/22/2024 08:26:27 - INFO - __main__ -   Prompt:
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:26:27 - INFO - __main__ -   Gold: yes
03/22/2024 08:26:27 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is no, Gandalf does not defeat Rincewin.\nReview: The answer to each of these questions is either yes or no. The reasoning is the same as the reasoning in the previous example.\n1. Is it possible to have a 100-year-old baby?\n2. Is there a 50-foot-tall giraffe?\n3. Is the Earth flat?\n4. Is a pig a mammal?\n5. Is an elephant a mollusk?\n6. Is water a solid?\n7. Is 12 a prime number?\n8. Is “The Lord of the Rings” a book?\n9. Is', '1) Gandalf is a witch. 2) Rincewinds is a sorcerer. 3) Witches are stronger than sorcerers. Thus Gandalf defeats Gandalf. So, the answer to the question is no, Gandalf does not defeat Rincewin.\nReview: The answer to each of these questions is either yes or no. The reasoning is the same as the reasoning in the previous example.\n1. Is it possible to have a 100-year-old baby?\n2. Is there a 50-foot-tall giraffe?\n3. Is the Earth flat?\n4. Is a pig a mammal?\n5. Is an elephant a mollusk?\n6. Is water a solid?\n7. Is 12 a prime number?\n8. Is “The Lord of the Rings” a book?\n9. Is']
03/22/2024 08:26:27 - INFO - __main__ -   Sampling generations (strategy=temperature):
03/22/2024 08:26:35 - INFO - __main__ -   Example #2:
03/22/2024 08:26:35 - INFO - __main__ -   Prompt:
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Does ontology require a scalpel?
Reasoning: 
03/22/2024 08:26:35 - INFO - __main__ -   Gold: no
03/22/2024 08:26:35 - INFO - __main__ -   Predictions: ['1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus a scalpal is not required for ontology. So, the answer to the question is no, ontology does not require a surgery.\n1. What is the difference between a yes or no question and a question that requires a yes/no answer?\n2. What are the different types of questions that require a yes-no answer, and how do you answer them?\n3. What types of answers are appropriate for a yes no question?\n4. How do you determine if a question requires a Yes or a No answer?', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus a scalpal is not required for ontology. So, the answer to the question is no, ontology does not require a surgery.\n1. What is the difference between a yes or no question and a question that requires a yes/no answer?\n2. What are the different types of questions that require a yes-no answer, and how do you answer them?\n3. What types of answers are appropriate for a yes no question?\n4. How do you determine if a question requires a Yes or a No answer?']
03/22/2024 08:26:35 - INFO - __main__ -   {'accuracy.accuracy': 0.5}
03/22/2024 08:26:35 - INFO - __main__ -   Saved results to outputs/outputs/self_consistency/strategy_qa/eval_llama-2-7b-hf_strategy_qa__validation_2_fewshot_temperature_22_03_08_26_0/results.json
03/22/2024 08:26:35 - INFO - __main__ -   Sampling generations (strategy=topk):
03/22/2024 08:33:16 - INFO - __main__ -   Script arguments:
03/22/2024 08:33:16 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 5, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:33:20 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:33:24 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 5, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': '', 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:33:24 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:36:30 - INFO - __main__ -   Script arguments:
03/22/2024 08:36:30 - INFO - __main__ -   {'out_dir': 'outputs/self_consistency', 'clean_out_dir': 'True', 'debug': True, 'verbose': True, 'seed': 42, 'run_id': 0, 'model': 'meta-llama/Llama-2-7b-hf', 'is_chat': False, 'load_in_8bit': False, 'eval_inf_fn_key': 'fewshot', 'eval_split': 'validation', 'dataset_name': 'strategy_qa', 'dataset_subname': 'None', 'eval_n_samples': 5, 'eval_retrieval_strategy': 'random', 'eval_output_sampling_strategy': 'all', 'eval_output_beam_size': 1, 'dataset_sample_strategy': 'random', 'eval_output_temperature': None, 'eval_output_top_k': None, 'eval_output_top_p': None}
03/22/2024 08:36:34 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
03/22/2024 08:36:38 - INFO - __main__ -   Eval arguments: {'inf_fn_key': 'fewshot', 'split': 'validation', 'metrics': [{'name': 'accuracy', 'score_keys': ['accuracy'], 'args': {}}], 'n_samples': 5, 'task_name': 'strategy_qa', 'dataset_sample_strategy': 'random', 'dataset_name': 'strategy_qa', 'dataset_subname': '', 'output_sampling_strategy': 'all', 'verbose': True, 'out_dir': 'outputs/outputs/self_consistency/strategy_qa', 'n_shots': None, 'retrieval_strategy': 'random', 'run_id': '0', 'inf_fn_kwargs': {}}
03/22/2024 08:36:38 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:36:43 - INFO - __main__ -   Example #1:
03/22/2024 08:36:43 - INFO - __main__ -   Prompt:
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:36:43 - INFO - __main__ -   Gold: yes
03/22/2024 08:36:43 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see above). Thus, Gandalf would defeat Rincewise in a battle. So, the answer would be yes.', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see above). Thus, Gandalf would defeat Rincewise in a battle. So, the answer would be yes.', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see above). Thus, Gandalf would defeat Rincewise in a battle. So, the answer would be yes.', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see above). Thus, Gandalf would defeat Rincewise in a battle. So, the answer would be yes.', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see above). Thus, Gandalf would defeat Rincewise in a battle. So, the answer would be yes.']
03/22/2024 08:36:43 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:36:52 - INFO - __main__ -   Example #2:
03/22/2024 08:36:52 - INFO - __main__ -   Prompt:
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Does ontology require a scalpel?
Reasoning: 
03/22/2024 08:36:52 - INFO - __main__ -   Gold: no
03/22/2024 08:36:52 - INFO - __main__ -   Predictions: ['1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, and 4 are true. 6) Therefore, ontology requires a scalpal. So, the answer to the question is yes, ontolgy requires a surgcal.\n1. What is the difference between a yes or no question and a question that requires a yes/no answer?\n2. What are the advantages of using a yes-no question?\n3. What types of questions can be answered with a yes, no, or maybe?\n4. What type of question is a yes no question?', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, and 4 are true. 6) Therefore, ontology requires a scalpal. So, the answer to the question is yes, ontolgy requires a surgcal.\n1. What is the difference between a yes or no question and a question that requires a yes/no answer?\n2. What are the advantages of using a yes-no question?\n3. What types of questions can be answered with a yes, no, or maybe?\n4. What type of question is a yes no question?', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, and 4 are true. 6) Therefore, ontology requires a scalpal. So, the answer to the question is yes, ontolgy requires a surgcal.\n1. What is the difference between a yes or no question and a question that requires a yes/no answer?\n2. What are the advantages of using a yes-no question?\n3. What types of questions can be answered with a yes, no, or maybe?\n4. What type of question is a yes no question?', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, and 4 are true. 6) Therefore, ontology requires a scalpal. So, the answer to the question is yes, ontolgy requires a surgcal.\n1. What is the difference between a yes or no question and a question that requires a yes/no answer?\n2. What are the advantages of using a yes-no question?\n3. What types of questions can be answered with a yes, no, or maybe?\n4. What type of question is a yes no question?', '1) Ontology is the study of being. 2) A scalpel is a surgical instrument. 3) Surgery is the treatment of disease by cutting. 4) Disease is the state of being unhealthy. 5) Being unheathy is not being. Thus 1, 2, 3, and 4 are true. 6) Therefore, ontology requires a scalpal. So, the answer to the question is yes, ontolgy requires a surgcal.\n1. What is the difference between a yes or no question and a question that requires a yes/no answer?\n2. What are the advantages of using a yes-no question?\n3. What types of questions can be answered with a yes, no, or maybe?\n4. What type of question is a yes no question?']
03/22/2024 08:36:52 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:37:01 - INFO - __main__ -   Example #3:
03/22/2024 08:37:01 - INFO - __main__ -   Prompt:
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Is it normal to find parsley in multiple sections of the grocery store?
Reasoning: 
03/22/2024 08:37:01 - INFO - __main__ -   Gold: yes
03/22/2024 08:37:01 - INFO - __main__ -   Predictions: ['1) Parsley is a herb. Herbs are in the produce section. 2) Parsnip is a root vegetable. Root vegetables are in produce. 3) Parrot is a bird. Birds are in pet food. Thus parsnips are in multiple groc. sections. So, the answer to the question is yes, it is normal to see parsely in multiple produce sections.\nLike the previous example, answer this question with a yes or no, and then provide the reason for your answer.\nRefer to the following table.\nWhich of the following is a true statement?\nA. The average of the first three numbers is 2.\nB. The sum of the last three numbers equals 12.', '1) Parsley is a herb. Herbs are in the produce section. 2) Parsnip is a root vegetable. Root vegetables are in produce. 3) Parrot is a bird. Birds are in pet food. Thus parsnips are in multiple groc. sections. So, the answer to the question is yes, it is normal to see parsely in multiple produce sections.\nLike the previous example, answer this question with a yes or no, and then provide the reason for your answer.\nRefer to the following table.\nWhich of the following is a true statement?\nA. The average of the first three numbers is 2.\nB. The sum of the last three numbers equals 12.', '1) Parsley is a herb. Herbs are in the produce section. 2) Parsnip is a root vegetable. Root vegetables are in produce. 3) Parrot is a bird. Birds are in pet food. Thus parsnips are in multiple groc. sections. So, the answer to the question is yes, it is normal to see parsely in multiple produce sections.\nLike the previous example, answer this question with a yes or no, and then provide the reason for your answer.\nRefer to the following table.\nWhich of the following is a true statement?\nA. The average of the first three numbers is 2.\nB. The sum of the last three numbers equals 12.', '1) Parsley is a herb. Herbs are in the produce section. 2) Parsnip is a root vegetable. Root vegetables are in produce. 3) Parrot is a bird. Birds are in pet food. Thus parsnips are in multiple groc. sections. So, the answer to the question is yes, it is normal to see parsely in multiple produce sections.\nLike the previous example, answer this question with a yes or no, and then provide the reason for your answer.\nRefer to the following table.\nWhich of the following is a true statement?\nA. The average of the first three numbers is 2.\nB. The sum of the last three numbers equals 12.', '1) Parsley is a herb. Herbs are in the produce section. 2) Parsnip is a root vegetable. Root vegetables are in produce. 3) Parrot is a bird. Birds are in pet food. Thus parsnips are in multiple groc. sections. So, the answer to the question is yes, it is normal to see parsely in multiple produce sections.\nLike the previous example, answer this question with a yes or no, and then provide the reason for your answer.\nRefer to the following table.\nWhich of the following is a true statement?\nA. The average of the first three numbers is 2.\nB. The sum of the last three numbers equals 12.']
03/22/2024 08:37:01 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:37:11 - INFO - __main__ -   Example #4:
03/22/2024 08:37:11 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Has Johns Hopkins University always treated subjects ethically?
Reasoning: 
03/22/2024 08:37:11 - INFO - __main__ -   Gold: no
03/22/2024 08:37:11 - INFO - __main__ -   Predictions: ['1906 is the year Johns\' Hopkins Hospital was founded. Johns’ Hopkins was the first hospital to treat patients ethically. Thus Johns-Hopkins University has always treated its subjects ethical. So, the answer to the question is yes, Johns–Hopkin\'s University has treated its patients ethical since its founding.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to say "yes."\nThe ability to answer "yes," "no," or "maybe" is a powerful tool. It is a tool that can be used to make decisions, to communicate, and to build relationships.\nThe Power of Yes\nThe first step to using the power of yes is to understand what it is. The power of the yes is the power to say yes. It\'s the power that allows you to say, "Yes, I can do that." It', '1906 is the year Johns\' Hopkins Hospital was founded. Johns’ Hopkins was the first hospital to treat patients ethically. Thus Johns-Hopkins University has always treated its subjects ethical. So, the answer to the question is yes, Johns–Hopkin\'s University has treated its patients ethical since its founding.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to say "yes."\nThe ability to answer "yes," "no," or "maybe" is a powerful tool. It is a tool that can be used to make decisions, to communicate, and to build relationships.\nThe Power of Yes\nThe first step to using the power of yes is to understand what it is. The power of the yes is the power to say yes. It\'s the power that allows you to say, "Yes, I can do that." It', '1906 is the year Johns\' Hopkins Hospital was founded. Johns’ Hopkins was the first hospital to treat patients ethically. Thus Johns-Hopkins University has always treated its subjects ethical. So, the answer to the question is yes, Johns–Hopkin\'s University has treated its patients ethical since its founding.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to say "yes."\nThe ability to answer "yes," "no," or "maybe" is a powerful tool. It is a tool that can be used to make decisions, to communicate, and to build relationships.\nThe Power of Yes\nThe first step to using the power of yes is to understand what it is. The power of the yes is the power to say yes. It\'s the power that allows you to say, "Yes, I can do that." It', '1906 is the year Johns\' Hopkins Hospital was founded. Johns’ Hopkins was the first hospital to treat patients ethically. Thus Johns-Hopkins University has always treated its subjects ethical. So, the answer to the question is yes, Johns–Hopkin\'s University has treated its patients ethical since its founding.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to say "yes."\nThe ability to answer "yes," "no," or "maybe" is a powerful tool. It is a tool that can be used to make decisions, to communicate, and to build relationships.\nThe Power of Yes\nThe first step to using the power of yes is to understand what it is. The power of the yes is the power to say yes. It\'s the power that allows you to say, "Yes, I can do that." It', '1906 is the year Johns\' Hopkins Hospital was founded. Johns’ Hopkins was the first hospital to treat patients ethically. Thus Johns-Hopkins University has always treated its subjects ethical. So, the answer to the question is yes, Johns–Hopkin\'s University has treated its patients ethical since its founding.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to say "yes."\nThe ability to answer "yes," "no," or "maybe" is a powerful tool. It is a tool that can be used to make decisions, to communicate, and to build relationships.\nThe Power of Yes\nThe first step to using the power of yes is to understand what it is. The power of the yes is the power to say yes. It\'s the power that allows you to say, "Yes, I can do that." It']
03/22/2024 08:37:11 - INFO - __main__ -   Sampling generations (strategy=arithmetic):
03/22/2024 08:37:22 - INFO - __main__ -   Example #5:
03/22/2024 08:37:22 - INFO - __main__ -   Prompt:
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Did Elle Fanning play an essential part in ending apartheid?
Reasoning: 
03/22/2024 08:37:22 - INFO - __main__ -   Gold: no
03/22/2024 08:37:22 - INFO - __main__ -   Predictions: ['1994 was the year that apartheid ended in South Africa. Elle Fannings’s first film was in 2001. Thus Elle Fannon could not have played an essential role in ending Apartheid. So, the answer to this question is no, Elle Fanner did not play an important role in the end of apartheid.\nReview\nIn this chapter, you learned how to answer questions with Yes and No. You also learned how the answer could be Yes or no, and how to provide the reason for your answer.\nIn the next chapter, we will learn how to use the word "because" to explain why something is true.\n# Chapter 10. Using "Because"\nIn This Chapter\n  * Using "because."\n  How to use "because," and how not to use it.\n  Using "since" and "because of."\nIn English, we use the words "because", "', '1994 was the year that apartheid ended in South Africa. Elle Fannings’s first film was in 2001. Thus Elle Fannon could not have played an essential role in ending Apartheid. So, the answer to this question is no, Elle Fanner did not play an important role in the end of apartheid.\nReview\nIn this chapter, you learned how to answer questions with Yes and No. You also learned how the answer could be Yes or no, and how to provide the reason for your answer.\nIn the next chapter, we will learn how to use the word "because" to explain why something is true.\n# Chapter 10. Using "Because"\nIn This Chapter\n  * Using "because."\n  How to use "because," and how not to use it.\n  Using "since" and "because of."\nIn English, we use the words "because", "', '1994 was the year that apartheid ended in South Africa. Elle Fannings’s first film was in 2001. Thus Elle Fannon could not have played an essential role in ending Apartheid. So, the answer to this question is no, Elle Fanner did not play an important role in the end of apartheid.\nReview\nIn this chapter, you learned how to answer questions with Yes and No. You also learned how the answer could be Yes or no, and how to provide the reason for your answer.\nIn the next chapter, we will learn how to use the word "because" to explain why something is true.\n# Chapter 10. Using "Because"\nIn This Chapter\n  * Using "because."\n  How to use "because," and how not to use it.\n  Using "since" and "because of."\nIn English, we use the words "because", "', '1994 was the year that apartheid ended in South Africa. Elle Fannings’s first film was in 2001. Thus Elle Fannon could not have played an essential role in ending Apartheid. So, the answer to this question is no, Elle Fanner did not play an important role in the end of apartheid.\nReview\nIn this chapter, you learned how to answer questions with Yes and No. You also learned how the answer could be Yes or no, and how to provide the reason for your answer.\nIn the next chapter, we will learn how to use the word "because" to explain why something is true.\n# Chapter 10. Using "Because"\nIn This Chapter\n  * Using "because."\n  How to use "because," and how not to use it.\n  Using "since" and "because of."\nIn English, we use the words "because", "', '1994 was the year that apartheid ended in South Africa. Elle Fannings’s first film was in 2001. Thus Elle Fannon could not have played an essential role in ending Apartheid. So, the answer to this question is no, Elle Fanner did not play an important role in the end of apartheid.\nReview\nIn this chapter, you learned how to answer questions with Yes and No. You also learned how the answer could be Yes or no, and how to provide the reason for your answer.\nIn the next chapter, we will learn how to use the word "because" to explain why something is true.\n# Chapter 10. Using "Because"\nIn This Chapter\n  * Using "because."\n  How to use "because," and how not to use it.\n  Using "since" and "because of."\nIn English, we use the words "because", "']
03/22/2024 08:37:22 - INFO - __main__ -   {'accuracy.accuracy': 0.2}
03/22/2024 08:37:22 - INFO - __main__ -   Saved results to outputs/outputs/self_consistency/strategy_qa/eval_llama-2-7b-hf_strategy_qa__validation_5_fewshot_arithmetic_22_03_08_37_0/results.json
03/22/2024 08:37:22 - INFO - __main__ -   Sampling generations (strategy=sample):
03/22/2024 08:37:32 - INFO - __main__ -   Example #1:
03/22/2024 08:37:32 - INFO - __main__ -   Prompt:
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Gandalf hypothetically defeats Rincewind in a wizard battle?
Reasoning: 
03/22/2024 08:37:32 - INFO - __main__ -   Gold: yes
03/22/2024 08:37:32 - INFO - __main__ -   Predictions: ['1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see the movie). Thus, Gandalf defeats him. So, the answer to the question is yes, Gandolf defeats the bumbly wizard Rincewing.\nReview\nThe following are some of the most common types of questions that you will see on the GRE.\n  * **Yes/No Questions:** These questions ask you to answer a question with a yes or no. The answer is either yes or it is no, and there is no in-between.\nExample: Is the answer yes or is the answer no?\n  Yes/No questions are the easiest type of question to answer.\n**_Answer:_** Yes\n  Example: Is this answer yes?\n**Answer:**', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see the movie). Thus, Gandalf defeats him. So, the answer to the question is yes, Gandolf defeats the bumbly wizard Rincewing.\nReview\nThe following are some of the most common types of questions that you will see on the GRE.\n  * **Yes/No Questions:** These questions ask you to answer a question with a yes or no. The answer is either yes or it is no, and there is no in-between.\nExample: Is the answer yes or is the answer no?\n  Yes/No questions are the easiest type of question to answer.\n**_Answer:_** Yes\n  Example: Is this answer yes?\n**Answer:**', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see the movie). Thus, Gandalf defeats him. So, the answer to the question is yes, Gandolf defeats the bumbly wizard Rincewing.\nReview\nThe following are some of the most common types of questions that you will see on the GRE.\n  * **Yes/No Questions:** These questions ask you to answer a question with a yes or no. The answer is either yes or it is no, and there is no in-between.\nExample: Is the answer yes or is the answer no?\n  Yes/No questions are the easiest type of question to answer.\n**_Answer:_** Yes\n  Example: Is this answer yes?\n**Answer:**', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see the movie). Thus, Gandalf defeats him. So, the answer to the question is yes, Gandolf defeats the bumbly wizard Rincewing.\nReview\nThe following are some of the most common types of questions that you will see on the GRE.\n  * **Yes/No Questions:** These questions ask you to answer a question with a yes or no. The answer is either yes or it is no, and there is no in-between.\nExample: Is the answer yes or is the answer no?\n  Yes/No questions are the easiest type of question to answer.\n**_Answer:_** Yes\n  Example: Is this answer yes?\n**Answer:**', '1) Gandalf is a witch-king. 2) Rincewinds is a bumbling wizard. 3) Gandolf is a powerful wizard, and Rincewin is a weak wizard (see the movie). Thus, Gandalf defeats him. So, the answer to the question is yes, Gandolf defeats the bumbly wizard Rincewing.\nReview\nThe following are some of the most common types of questions that you will see on the GRE.\n  * **Yes/No Questions:** These questions ask you to answer a question with a yes or no. The answer is either yes or it is no, and there is no in-between.\nExample: Is the answer yes or is the answer no?\n  Yes/No questions are the easiest type of question to answer.\n**_Answer:_** Yes\n  Example: Is this answer yes?\n**Answer:**']
03/22/2024 08:37:32 - INFO - __main__ -   Sampling generations (strategy=sample):
03/22/2024 08:37:42 - INFO - __main__ -   Example #2:
03/22/2024 08:37:42 - INFO - __main__ -   Prompt:
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Does ontology require a scalpel?
Reasoning: 
03/22/2024 08:37:42 - INFO - __main__ -   Gold: no
03/22/2024 08:37:42 - INFO - __main__ -   Predictions: ['1. Ontology is the study of being. 2. A scalpel is a surgical instrument. 3. Surgery is the treatment of disease. 4. Disease is the absence of being (health). 5. Thus ontology requires a scalpal. So, the answer to the question is yes, ontology does require a surgery.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to answer a question with a simple "yes." The power of yes is a powerful tool for the following reasons:\n  * It is a simple answer.\n  **1.** It is easy to understand.\n**2.** The answer is clear.\nIt is a clear answer. It is not a "yes, but" answer. A "yes but" is an answer that begins with "yes," but then goes on to explain why the answer should be "no', '1. Ontology is the study of being. 2. A scalpel is a surgical instrument. 3. Surgery is the treatment of disease. 4. Disease is the absence of being (health). 5. Thus ontology requires a scalpal. So, the answer to the question is yes, ontology does require a surgery.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to answer a question with a simple "yes." The power of yes is a powerful tool for the following reasons:\n  * It is a simple answer.\n  **1.** It is easy to understand.\n**2.** The answer is clear.\nIt is a clear answer. It is not a "yes, but" answer. A "yes but" is an answer that begins with "yes," but then goes on to explain why the answer should be "no', '1. Ontology is the study of being. 2. A scalpel is a surgical instrument. 3. Surgery is the treatment of disease. 4. Disease is the absence of being (health). 5. Thus ontology requires a scalpal. So, the answer to the question is yes, ontology does require a surgery.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to answer a question with a simple "yes." The power of yes is a powerful tool for the following reasons:\n  * It is a simple answer.\n  **1.** It is easy to understand.\n**2.** The answer is clear.\nIt is a clear answer. It is not a "yes, but" answer. A "yes but" is an answer that begins with "yes," but then goes on to explain why the answer should be "no', '1. Ontology is the study of being. 2. A scalpel is a surgical instrument. 3. Surgery is the treatment of disease. 4. Disease is the absence of being (health). 5. Thus ontology requires a scalpal. So, the answer to the question is yes, ontology does require a surgery.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to answer a question with a simple "yes." The power of yes is a powerful tool for the following reasons:\n  * It is a simple answer.\n  **1.** It is easy to understand.\n**2.** The answer is clear.\nIt is a clear answer. It is not a "yes, but" answer. A "yes but" is an answer that begins with "yes," but then goes on to explain why the answer should be "no', '1. Ontology is the study of being. 2. A scalpel is a surgical instrument. 3. Surgery is the treatment of disease. 4. Disease is the absence of being (health). 5. Thus ontology requires a scalpal. So, the answer to the question is yes, ontology does require a surgery.\n# 10\n# The Power of "Yes"\nThe power of "yes" is the ability to answer a question with a simple "yes." The power of yes is a powerful tool for the following reasons:\n  * It is a simple answer.\n  **1.** It is easy to understand.\n**2.** The answer is clear.\nIt is a clear answer. It is not a "yes, but" answer. A "yes but" is an answer that begins with "yes," but then goes on to explain why the answer should be "no']
03/22/2024 08:37:42 - INFO - __main__ -   Sampling generations (strategy=sample):
03/22/2024 08:37:53 - INFO - __main__ -   Example #3:
03/22/2024 08:37:53 - INFO - __main__ -   Prompt:
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Is it normal to find parsley in multiple sections of the grocery store?
Reasoning: 
03/22/2024 08:37:53 - INFO - __main__ -   Gold: yes
03/22/2024 08:37:53 - INFO - __main__ -   Predictions: ['1) Parsley is a herb. Herbs are found in the produce section. 2) Parsnip is a root vegetable. Root vegetables are found with other root vegetables. 3) Parrot is a bird. Birds are found near the pet food. 4) Parakeet is a pet. Pets are found at the pet store. 5) Parachute is a piece of equipment. Equipment is found in sports. 6) Parasite is a disease. Diseases are found under health. 7) Paranoia is a mental illness. Mental illnesses are found next to the pharmacy. 8) Paraplegic is a person who cannot walk. People who cannot move are found on the first floor. 9) Paralyzed is a state of being unable to move. People in this state are found by the hospital. \nAnswer\nQuestion\n', '1) Parsley is a herb. Herbs are found in the produce section. 2) Parsnip is a root vegetable. Root vegetables are found with other root vegetables. 3) Parrot is a bird. Birds are found near the pet food. 4) Parakeet is a pet. Pets are found at the pet store. 5) Parachute is a piece of equipment. Equipment is found in sports. 6) Parasite is a disease. Diseases are found under health. 7) Paranoia is a mental illness. Mental illnesses are found next to the pharmacy. 8) Paraplegic is a person who cannot walk. People who cannot move are found on the first floor. 9) Paralyzed is a state of being unable to move. People in this state are found by the hospital. \nAnswer\nQuestion\n', '1) Parsley is a herb. Herbs are found in the produce section. 2) Parsnip is a root vegetable. Root vegetables are found with other root vegetables. 3) Parrot is a bird. Birds are found near the pet food. 4) Parakeet is a pet. Pets are found at the pet store. 5) Parachute is a piece of equipment. Equipment is found in sports. 6) Parasite is a disease. Diseases are found under health. 7) Paranoia is a mental illness. Mental illnesses are found next to the pharmacy. 8) Paraplegic is a person who cannot walk. People who cannot move are found on the first floor. 9) Paralyzed is a state of being unable to move. People in this state are found by the hospital. \nAnswer\nQuestion\n', '1) Parsley is a herb. Herbs are found in the produce section. 2) Parsnip is a root vegetable. Root vegetables are found with other root vegetables. 3) Parrot is a bird. Birds are found near the pet food. 4) Parakeet is a pet. Pets are found at the pet store. 5) Parachute is a piece of equipment. Equipment is found in sports. 6) Parasite is a disease. Diseases are found under health. 7) Paranoia is a mental illness. Mental illnesses are found next to the pharmacy. 8) Paraplegic is a person who cannot walk. People who cannot move are found on the first floor. 9) Paralyzed is a state of being unable to move. People in this state are found by the hospital. \nAnswer\nQuestion\n', '1) Parsley is a herb. Herbs are found in the produce section. 2) Parsnip is a root vegetable. Root vegetables are found with other root vegetables. 3) Parrot is a bird. Birds are found near the pet food. 4) Parakeet is a pet. Pets are found at the pet store. 5) Parachute is a piece of equipment. Equipment is found in sports. 6) Parasite is a disease. Diseases are found under health. 7) Paranoia is a mental illness. Mental illnesses are found next to the pharmacy. 8) Paraplegic is a person who cannot walk. People who cannot move are found on the first floor. 9) Paralyzed is a state of being unable to move. People in this state are found by the hospital. \nAnswer\nQuestion\n']
03/22/2024 08:37:53 - INFO - __main__ -   Sampling generations (strategy=sample):
03/22/2024 08:38:03 - INFO - __main__ -   Example #4:
03/22/2024 08:38:03 - INFO - __main__ -   Prompt:
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Could Brooke Shields succeed at University of Pennsylvania?
Reasoning: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Has Johns Hopkins University always treated subjects ethically?
Reasoning: 
03/22/2024 08:38:03 - INFO - __main__ -   Gold: no
03/22/2024 08:38:03 - INFO - __main__ -   Predictions: ["1906 is the year Johns's Hopkins Hospital was founded. Johns was a Quaker. Quakers are known for their ethical treatment of subjects. Thus Johns would have treated subjects at Johns’s Hopkin’s Hospital ethically. So, the answer to the question is yes, Johns has always treated his subjects ethical.\n1.2.2 The Reasoning\nThe reasoning is the most important part of the answer. The reasoning is what makes the answer correct. The answer is correct if the reasoning is correct.\nThe following are examples of correct reasoning.\nExample 1\nQuestion 1: Is the Earth flat?\nAnswer 1.1: No.\nReasons 1a: The Earth is not flat.\nExplanation 1b: The shape of the Earth is spherical. The Earth’s shape is not the same as a flat surface.\n\nExample: 2\n", "1906 is the year Johns's Hopkins Hospital was founded. Johns was a Quaker. Quakers are known for their ethical treatment of subjects. Thus Johns would have treated subjects at Johns’s Hopkin’s Hospital ethically. So, the answer to the question is yes, Johns has always treated his subjects ethical.\n1.2.2 The Reasoning\nThe reasoning is the most important part of the answer. The reasoning is what makes the answer correct. The answer is correct if the reasoning is correct.\nThe following are examples of correct reasoning.\nExample 1\nQuestion 1: Is the Earth flat?\nAnswer 1.1: No.\nReasons 1a: The Earth is not flat.\nExplanation 1b: The shape of the Earth is spherical. The Earth’s shape is not the same as a flat surface.\n\nExample: 2\n", "1906 is the year Johns's Hopkins Hospital was founded. Johns was a Quaker. Quakers are known for their ethical treatment of subjects. Thus Johns would have treated subjects at Johns’s Hopkin’s Hospital ethically. So, the answer to the question is yes, Johns has always treated his subjects ethical.\n1.2.2 The Reasoning\nThe reasoning is the most important part of the answer. The reasoning is what makes the answer correct. The answer is correct if the reasoning is correct.\nThe following are examples of correct reasoning.\nExample 1\nQuestion 1: Is the Earth flat?\nAnswer 1.1: No.\nReasons 1a: The Earth is not flat.\nExplanation 1b: The shape of the Earth is spherical. The Earth’s shape is not the same as a flat surface.\n\nExample: 2\n", "1906 is the year Johns's Hopkins Hospital was founded. Johns was a Quaker. Quakers are known for their ethical treatment of subjects. Thus Johns would have treated subjects at Johns’s Hopkin’s Hospital ethically. So, the answer to the question is yes, Johns has always treated his subjects ethical.\n1.2.2 The Reasoning\nThe reasoning is the most important part of the answer. The reasoning is what makes the answer correct. The answer is correct if the reasoning is correct.\nThe following are examples of correct reasoning.\nExample 1\nQuestion 1: Is the Earth flat?\nAnswer 1.1: No.\nReasons 1a: The Earth is not flat.\nExplanation 1b: The shape of the Earth is spherical. The Earth’s shape is not the same as a flat surface.\n\nExample: 2\n", "1906 is the year Johns's Hopkins Hospital was founded. Johns was a Quaker. Quakers are known for their ethical treatment of subjects. Thus Johns would have treated subjects at Johns’s Hopkin’s Hospital ethically. So, the answer to the question is yes, Johns has always treated his subjects ethical.\n1.2.2 The Reasoning\nThe reasoning is the most important part of the answer. The reasoning is what makes the answer correct. The answer is correct if the reasoning is correct.\nThe following are examples of correct reasoning.\nExample 1\nQuestion 1: Is the Earth flat?\nAnswer 1.1: No.\nReasons 1a: The Earth is not flat.\nExplanation 1b: The shape of the Earth is spherical. The Earth’s shape is not the same as a flat surface.\n\nExample: 2\n"]
03/22/2024 08:38:03 - INFO - __main__ -   Sampling generations (strategy=sample):
03/22/2024 08:38:13 - INFO - __main__ -   Example #5:
03/22/2024 08:38:13 - INFO - __main__ -   Prompt:
Question: Would a pear sink in water?
Reasoning: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.
Answer: no
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Reasoning: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.
Answer: no
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Reasoning: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.
Answer: no
Question: Is it common to see frost during some college commencements?
Reasoning: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.
Answer: yes
Question: Do hamsters provide food for any animals?
Reasoning: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.
Answer: yes
Like each of the previous examples, answer the following question with Yes or No, and provide the reasoning as demonstrated in the examples.
Question: Did Elle Fanning play an essential part in ending apartheid?
Reasoning: 
03/22/2024 08:38:13 - INFO - __main__ -   Gold: no
03/22/2024 08:38:13 - INFO - __main__ -   Predictions: ['1994 was the year apartheid ended. Elle Fannings’s first movie was in 2001. Thus Elle Fannon did not play an important part in the ending of apartheid. So, the answer to the question is no, Elle Fanner did not end apartheid in 16.\nReview the following questions and answers.\n1. Is it possible to have a 100-year-old baby?\n2. Is the number of people who have died from COVID-19 greater than the number who have been born in 50 years?\n3. Is there a greater chance of being struck by lightning than being born in the United States?\n4. Is a 2-year old more likely to be struck by a car than a 90- year-old?\n5. Is 12 years old the average age of a person who has died from a shark attack?\n', '1994 was the year apartheid ended. Elle Fannings’s first movie was in 2001. Thus Elle Fannon did not play an important part in the ending of apartheid. So, the answer to the question is no, Elle Fanner did not end apartheid in 16.\nReview the following questions and answers.\n1. Is it possible to have a 100-year-old baby?\n2. Is the number of people who have died from COVID-19 greater than the number who have been born in 50 years?\n3. Is there a greater chance of being struck by lightning than being born in the United States?\n4. Is a 2-year old more likely to be struck by a car than a 90- year-old?\n5. Is 12 years old the average age of a person who has died from a shark attack?\n', '1994 was the year apartheid ended. Elle Fannings’s first movie was in 2001. Thus Elle Fannon did not play an important part in the ending of apartheid. So, the answer to the question is no, Elle Fanner did not end apartheid in 16.\nReview the following questions and answers.\n1. Is it possible to have a 100-year-old baby?\n2. Is the number of people who have died from COVID-19 greater than the number who have been born in 50 years?\n3. Is there a greater chance of being struck by lightning than being born in the United States?\n4. Is a 2-year old more likely to be struck by a car than a 90- year-old?\n5. Is 12 years old the average age of a person who has died from a shark attack?\n', '1994 was the year apartheid ended. Elle Fannings’s first movie was in 2001. Thus Elle Fannon did not play an important part in the ending of apartheid. So, the answer to the question is no, Elle Fanner did not end apartheid in 16.\nReview the following questions and answers.\n1. Is it possible to have a 100-year-old baby?\n2. Is the number of people who have died from COVID-19 greater than the number who have been born in 50 years?\n3. Is there a greater chance of being struck by lightning than being born in the United States?\n4. Is a 2-year old more likely to be struck by a car than a 90- year-old?\n5. Is 12 years old the average age of a person who has died from a shark attack?\n', '1994 was the year apartheid ended. Elle Fannings’s first movie was in 2001. Thus Elle Fannon did not play an important part in the ending of apartheid. So, the answer to the question is no, Elle Fanner did not end apartheid in 16.\nReview the following questions and answers.\n1. Is it possible to have a 100-year-old baby?\n2. Is the number of people who have died from COVID-19 greater than the number who have been born in 50 years?\n3. Is there a greater chance of being struck by lightning than being born in the United States?\n4. Is a 2-year old more likely to be struck by a car than a 90- year-old?\n5. Is 12 years old the average age of a person who has died from a shark attack?\n']
03/22/2024 08:38:13 - INFO - __main__ -   {'accuracy.accuracy': 0.0}
03/22/2024 08:38:13 - INFO - __main__ -   Saved results to outputs/outputs/self_consistency/strategy_qa/eval_llama-2-7b-hf_strategy_qa__validation_5_fewshot_sample_22_03_08_38_0/results.json

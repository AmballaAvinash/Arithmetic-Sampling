{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b33947-e003-4fad-8ad7-d2ef443c1ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessor,LogitsProcessorList\n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a4a3db-76ad-4709-ad4e-0fb3f413dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_exemplars = [\n",
    "    {\n",
    "        \"question\": \"Do hamsters provide food for any animals?\",\n",
    "        \"reasoning\": \"Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.\",\n",
    "        \"answer\" : \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Could Brooke Shields succeed at University of Pennsylvania?\",\n",
    "        \"reasoning\": \"Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the answer is yes.\",\n",
    "        \"answer\" : \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Hydrogen’s atomic number squared exceeds number of Spice Girls?\",\n",
    "        \"reasoning\": \"Hydroßen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number squared is less than 5. So the answer is no.\",\n",
    "        \"answer\" : \"No\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is it common to see frost during some college commencements?\",\n",
    "        \"reasoning\": \"College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\",\n",
    "        \"answer\" : \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Could a llama birth twice during War in Vietnam (1945-46)?\",\n",
    "        \"reasoning\": \"The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\",\n",
    "        \"answer\" : \"No\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Would a pear sink in water?\",\n",
    "        \"reasoning\": \"The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float. Thus, a pear would float. So the answer is no.\",\n",
    "        \"answer\" : \"No\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ef31e7-88bb-4110-a333-fbf0efa153ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_experiments():\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=True)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2\").to('cuda')\n",
    "#     torch.device = 'cuda'\n",
    "#     # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "#     model.config.pad_token_id = model.config.eos_token_id\n",
    "#     model.generation_config.pad_token_id = model.config.eos_token_id\n",
    "#     data = random.sample(load_hf_data_set('validation','wmt14','de-en')['translation'],200)\n",
    "    \n",
    "#     default_fwd_instructi\n",
    "#     on = f\"Yes or No: {question}\"\n",
    "#     default_fwd_input_prefix = \"Yes or No: \"\n",
    "#     default_fwd_target_prefix = \"Answer: \"\n",
    "#     demos = random.sample(qa_pairs,5)\n",
    "#     demos = [[default_fwd_input_prefix + d[\"question\"]] +'\\n'  [default_fwd_target_prefix + d[\"answer\"]] for d in demos]\n",
    "#     prompt_arr = demos.append([default_fwd_instruction,default_fwd_input_prefix])\n",
    "#     for N in [5, 10]:\n",
    "#         for temp in [0.5, 1.0]:\n",
    "#             test(prompt_arr, model, tokenizer, data, default_fwd_target_prefix, N, temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e625d3-6e57-45af-819f-818403a41bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union,List\n",
    "\n",
    "class TruncateLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self,token_id: Union[int, List[int]],stop_word:Union[str, List[str]],eos_token_id: Union[int, List[int]],tokenizer):\n",
    "        if isinstance(token_id, int):\n",
    "            token_id = [token_id]\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        # if not all(isinstance(i,int) for i in token_id) or any(i < 0 for i in token_id):\n",
    "        #     logger.warning(f\"`token_id` has to be a list of positive integers, but is {token_id}\")\n",
    "        self.stop_word = stop_word\n",
    "        self.token_id = token_id\n",
    "        \n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self,input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        \n",
    "        # print(cur_len)\n",
    "        # print(scores.shape)\n",
    "        max_score_ids = torch.argmax(scores, dim=1)\n",
    "        \n",
    "        for i in range(len(max_score_ids)):\n",
    "            #print(tokenizer.decode(max_score_ids[i]))\n",
    "            if (max_score_ids[i] in tokenizer('Question').input_ids) :\n",
    "                scores[i][:] = -float('inf')\n",
    "                scores[i][self.eos_token_id[0]] = 0\n",
    "        #print(torch.argmax(scores, dim=1))\n",
    "        # print(input_ids.shape)\n",
    "        #print(input_ids)\n",
    "        scores.to(input_ids.device)\n",
    "        if torch.argmax(scores[:,]) in self.token_id:\n",
    "            print('yes')\n",
    "            scores = torch.zeros(scores.shape)\n",
    "            scores[:, self.eos_token_id] = 1\n",
    "                \n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96646422-6c66-427e-b148-14e68c525612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc79f666-c8f2-4fbf-b9e0-c23979b54c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2290\n",
      "2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:57<00:00, 28.92s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.75 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 41.52 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# device_map=\"auto\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;66;43;03m# torch_dtype=torch.bfloat16,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/work/pi_mccallum_umass_edu/aparashar_umass_edu/models/.cache\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;66;43;03m# device_map=\"auto\",\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                                \u001b[49m\u001b[38;5;66;43;03m# quantization_config=bnb_config\u001b[39;49;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m LogitsProcessorList(\n\u001b[1;32m     33\u001b[0m     [\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m#StopAfterTokenLogitsProcessor(token_id_to_stop=29973,max_length=40),\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         TruncateLogitsProcessor(stop_word\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m,token_id\u001b[38;5;241m=\u001b[39m[tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m)],eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,tokenizer\u001b[38;5;241m=\u001b[39mtokenizer),\n\u001b[1;32m     36\u001b[0m     ])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# torch.device = 'cuda'\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# set pad_token_id to eos_token_id because GPT2 does not have a EOS token\u001b[39;00m\n",
      "File \u001b[0;32m/work/pi_dhruveshpate_umass_edu/aparashar_umass_edu/transformers_local/src/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 44.35 GiB of which 34.75 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 41.52 GiB is allocated by PyTorch, and 2.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# default_instruction = \"Answer the question with Yes or No: \"\n",
    "default_fwd_input_prefix = \"\"\"Like each of the previous examples, answer the question with either \"Yes\" or \"No\".\n",
    "Provide reasoning for your answer.\n",
    "End your reasoning with the sentence: \"So the answer is <your_answer>\". Replace \"<your_answer>\" with your final answer, which should be either \"Yes\" or \"No\".\"\"\"\n",
    "\n",
    "default_fwd_demos_prefix = \"Here are some examples of reasoning based question answers.\"\n",
    "default_fwd_reasoning_prefix = \"Reasoning: \"\n",
    "default_fwd_question_prefix = \"Question: \"\n",
    "default_fwd_target_prefix = \"Answer: \"\n",
    "demos = random.sample(qa_exemplars,6)\n",
    "demos = [[default_fwd_question_prefix + d[\"question\"]]+ [default_fwd_reasoning_prefix + d[\"reasoning\"] + default_fwd_target_prefix + d[\"answer\"]] for d in demos]# def test(prompt_arr, model, tokenizer, data,demos, default_fwd_target_prefix, N = 10, temp = 0.5):\n",
    "bnb_config= BitsAndBytesConfig(\n",
    "            load_in_8bit=True,)\n",
    "output_dict = {}\n",
    "import json\n",
    "filepath = \"/work/pi_dhruveshpate_umass_edu/aparashar_umass_edu/Arithmetic-Sampling/data/stratqa_data.json\"\n",
    "with open(filepath,'r') as f:\n",
    "    strat_data = json.load(f)['examples']\n",
    "    \n",
    "demo_questions = [d['question'] for d in qa_exemplars]\n",
    "print(len(strat_data))\n",
    "strat_data = [i for i in strat_data if i['input'] not in demo_questions]\n",
    "print(len(strat_data))\n",
    "# device_map=\"auto\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             cache_dir = '/work/pi_mccallum_umass_edu/aparashar_umass_edu/models/.cache',\n",
    "                                             # device_map=\"auto\",\n",
    "                                                quantization_config=bnb_config\n",
    "                                            ).to('cuda')\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        #StopAfterTokenLogitsProcessor(token_id_to_stop=29973,max_length=40),\n",
    "        TruncateLogitsProcessor(stop_word='Question',token_id=[tokenizer.encode('Question')],eos_token_id=tokenizer.eos_token_id,tokenizer=tokenizer),\n",
    "    ])\n",
    "# torch.device = 'cuda'\n",
    "# set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.generation_config.pad_token_id = model.config.eos_token_id\n",
    "for idx, d in enumerate(tqdm(strat_data[1:7], desc=\"Predicting\")):\n",
    "    print(d)\n",
    "    prompt_arr = [default_fwd_input_prefix] +[default_fwd_question_prefix + d['input']]+[default_fwd_reasoning_prefix]\n",
    "    prompt_arr = demos + [prompt_arr]\n",
    "    prompt_arr = [element for sublist in prompt_arr for element in sublist]\n",
    "    # print(prompt_arr)\n",
    "    input_prompt =  ('\\n').join(prompt_arr)\n",
    "    print(f\"prompt: {input_prompt}\")\n",
    "    input_ids = tokenizer(input_prompt, truncation=True, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    outputs_arith = model.generate(\n",
    "        input_ids = input_ids,\n",
    "        num_return_sequences = 5,\n",
    "        do_sample = True,\n",
    "        max_new_tokens = 200,\n",
    "        temperature = 1.,\n",
    "        num_beams = 1,\n",
    "        return_dict_in_generate=True,\n",
    "        logits_processor=logits_processor,\n",
    "        use_arithmetic = True\n",
    "        )\n",
    "    outputs_sample = model.generate(\n",
    "        input_ids = input_ids,\n",
    "        num_return_sequences = 5,\n",
    "        do_sample = True,\n",
    "        temperature = 1.,\n",
    "        num_beams = 1,\n",
    "        max_new_tokens = 200,\n",
    "        return_dict_in_generate=True,\n",
    "        logits_processor=logits_processor,\n",
    "        \n",
    "        use_arithmetic = False\n",
    "        )\n",
    "    print(input_ids.shape[1])\n",
    "    outputs_arith = [tokenizer.decode(o, skip_special_tokens=True)[len(input_prompt):] for o in outputs_arith.sequences]\n",
    "    outputs_sample = [tokenizer.decode(o, skip_special_tokens=True)[len(input_prompt):] for o in outputs_sample.sequences]\n",
    "    # print(f\"Arithmetic output: {tokenizer.batch_decode(outputs_arith[input_ids.shape[1]:], skip_special_tokens=True)}\")\n",
    "    outputs_arith_str = '\\n###\\n'.join(outputs_arith)\n",
    "    print(\"---\")\n",
    "    outputs_sample_str = '\\n###\\n'.join(outputs_sample)\n",
    "    print(f\"Arithmetic output: {outputs_arith_str}\")\n",
    "    print(f\"Sample output: {outputs_sample_str}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e2e93-13a6-4596-8c4b-263868da9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filepath = \"/work/pi_dhruveshpate_umass_edu/aparashar_umass_edu/Arithmetic-Sampling/data/stratqa_data.json\"\n",
    "with open(filepath,'r') as f:\n",
    "    strat_data = json.load(f)['examples']\n",
    "demo_questions = [d['question'] for d in demos]\n",
    "print(len(strat_data))\n",
    "strat_data = [i for i in strat_data if i['input'] not in demo_questions]\n",
    "print(len(strat_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6a5172-6299-42b2-8ec0-b5d48e4c1696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"Hydrogen's atomic number squared exceeds number of Spice Girls?\",\n",
       " 'target_scores': {'Yes': 0, 'No': 1},\n",
       " 'target': 'No. Hydrogen is the first element and has an atomic number of one. To square a number, you multiply it by itself. The Spice Girls has five members.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4edae9-45b4-4fa1-ae66-8f7b6b596071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/pi_dhruveshpate_umass_edu/aparashar_umass_edu/Arithmetic-Sampling/notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f393ea3-a549-4ff0-a320-2e67eaf247f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Is it common to see frost during some college commencements?',\n",
       " 'target_scores': {'Yes': 1, 'No': 0},\n",
       " 'target': \"Yes. College commencement ceremonies often happen during the months of December, May, and sometimes June.  Frost isn't uncommon to see during the month of December, as it is the winter.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "626a8c15-fff1-4132-a89e-a8556772c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.43.0\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/TimDettmers/bitsandbytes\n",
      "Author: Tim Dettmers\n",
      "Author-email: dettmers@cs.washington.edu\n",
      "License: MIT\n",
      "Location: /work/pi_mccallum_umass_edu/aparashar_umass_edu/.conda/envs/as01/lib/python3.10/site-packages\n",
      "Requires: numpy, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show bitsandbytes  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b8d52-87e7-4158-a5b1-72974d7dd30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
